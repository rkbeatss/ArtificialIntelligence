{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 6 - Knowledge Representation and NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSI4106 Artificial Intelligence  \n",
    "Fall 2019  \n",
    "Prepared by Caroline BarriÃ¨re and Julian Templeton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***INTRODUCTION***:  \n",
    "\n",
    "In this notebook you will explore one of the techniques and one of the resources that you have seen in class for Name-Entity Recognition (NER). Instead of typical PER, LOC, ORG entity types, we will go into the restaurant world and focus on entities of type *Cuisine*, *Dish* and others.\n",
    "\n",
    "You will begin by loading in our train and test corpora of restaurant related sentences and move to using a Gazetteer for NER of the words in the corpus.   \n",
    "\n",
    "Then you will explore Wordnet, a lexical semantic network in which knowledge is organized by interrelated synsets (groups of synonyms). Using Wordnet you will use the hyponyms of the *dish* to recognize how to tag tokens as *Dish*.   \n",
    "\n",
    "An important note is that this notebook will be working with *Single Words* rather than *Multi-words*. This means that when tagging words you will not be able to get all of the tags for each word. For example, a sentence \"Find me a restaurant where they serve fettucini alfredo\" is a sentence where the tag *Cuisine* should be used for several words (fettucini alfredo). But looking at single words, only fettucini may be tagged correctly. Dealing with multi-words is quite complex, and that is why we limit ourselves to single words in this notebook, but keep in mind that we would typically want to consider several words, not just single words.\n",
    "\n",
    "Also, the evaluation in this notebook is more qualitative in nature.  We will not perform the usual quantitative methods of precision/recall, but rather just look at examples of output and discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***HOMEWORK***:  \n",
    "Go through the notebook by running each cell, one at a time.  \n",
    "Look for **(TO DO)** for the tasks that you need to perform. Do not edit the code outside of the questions which you are asked to answer unless specifically asked. Once you're done, Sign the notebook (at the end of the notebook), and submit it.  \n",
    "\n",
    "*The notebook will be marked on 15.  \n",
    "Each **(TO DO)** has a number of points associated with it.*\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # For the regular expressions that we will be using\n",
    "from nltk.corpus import wordnet # Import Wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Setting up the corpus**   \n",
    "Before working on our NER tasks we must setup the corpus that we will be using. For this notebook we will be working with a corpus related to restaurants.   \n",
    "\n",
    "The restaurant corpus is provided on Brightspace but is also available at: https://groups.csail.mit.edu/sls/downloads/ as the *MIT Restaurant Corpus*. This corpus provides lists of tokens from sentences, where each sentence is separated by a blank line, with each token provided a NER tag such as *Cuisine*, *Dish*, *Other (O)*, ... We will explore these soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin collecting the sentences and tags from restauranttrain.bio.txt\n",
    "corpus_train = []\n",
    "with open(\"restauranttrain.bio.txt\") as f:\n",
    "    corpus_train = f.readlines()\n",
    "    \n",
    "# Begin collecting the sentences and tags from restauranttest.bio.txt\n",
    "corpus_test = []\n",
    "with open(\"restauranttest.bio.txt\") as f:\n",
    "    corpus_test = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will convert the corpus into pairs of sentence tokens and their respective tags.\n",
    "# We will also limit the minimum length of sentences that we keep along with the total number\n",
    "# of sentences.\n",
    "def setup_sentences(corpus, min_sentence_len, max_sentences):\n",
    "    sentences = [] # All of our tokenized sentences with their respective NER tags\n",
    "    sentence = [[], []] # Storing the NER tags along with the sentences in two separate lists\n",
    "    # Format our corpus as a list of lists for easy use later\n",
    "    for line in corpus:\n",
    "        vals = line.split()\n",
    "        if (vals == []):\n",
    "            sentences.append(sentence)\n",
    "            sentence = [[], []]\n",
    "        else:\n",
    "            sentence[0].append(re.sub(r\"[A-Z]-\", \"\", vals[0])) # NER tag (we remove prefixes for simplicity)\n",
    "            sentence[1].append(vals[1]) # Sentence\n",
    "    # Now we will only keep sentences containing at least min_sentence_len words\n",
    "    sentences = [sentence for sentence in sentences if len(sentence[1]) >= min_sentence_len]\n",
    "    # Finally return up to max_sentences sentences\n",
    "    return sentences[:max_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = setup_sentences(corpus_train, 5, 1000)\n",
    "test_sentences = setup_sentences(corpus_test, 5, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "print(len(train_sentences))\n",
    "print(len(test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5', 'star', 'resturants', 'in', 'my', 'town'] \n",
      " ['Rating', 'Rating', 'O', 'Location', 'Location', 'Location'] \n",
      "\n",
      "['about', 'how', 'much', 'is', 'a', 'midpriced', 'bottle', 'of', 'good', 'wine', 'at', 'davidos', 'italian', 'palace'] \n",
      " ['O', 'O', 'O', 'O', 'O', 'Price', 'Rating', 'O', 'Rating', 'Cuisine', 'O', 'Restaurant_Name', 'Restaurant_Name', 'Restaurant_Name'] \n",
      "\n",
      "['are', 'there', 'any', 'authentic', 'vietnamese', 'restaurants', 'in', 'the', 'area', 'that', 'specialize', 'in', 'regional', 'dishes'] \n",
      " ['O', 'O', 'O', 'Cuisine', 'Cuisine', 'O', 'Location', 'Location', 'Location', 'O', 'O', 'O', 'Cuisine', 'O'] \n",
      "\n",
      "['can', 'i', 'get', 'a', 'list', 'of', 'restaurants', 'that', 'are', 'still', 'open', 'that', 'serve', 'pancakes'] \n",
      " ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'Hours', 'Hours', 'O', 'O', 'Dish'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let us take a look at a few sample sentences and their NER tags from train_sentences\n",
    "# We will discuss the tags themselves very soon!\n",
    "print(train_sentences[1][1], \"\\n\", train_sentences[1][0], \"\\n\")\n",
    "print(train_sentences[10][1], \"\\n\", train_sentences[10][0], \"\\n\")\n",
    "print(train_sentences[100][1], \"\\n\", train_sentences[100][0], \"\\n\")\n",
    "print(train_sentences[500][1], \"\\n\", train_sentences[500][0], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Understanding the corpus**   \n",
    "Let us explore the corpus to look at the kinds of NER tags being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating\n",
      "O\n",
      "Amenity\n",
      "Location\n",
      "Restaurant_Name\n",
      "Price\n",
      "Hours\n",
      "Dish\n",
      "Cuisine\n",
      "['Rating', 'O', 'Amenity', 'Location', 'Restaurant_Name', 'Price', 'Hours', 'Dish', 'Cuisine']\n",
      "{'Rating': True, 'O': True, 'Amenity': True, 'Location': True, 'Restaurant_Name': True, 'Price': True, 'Hours': True, 'Dish': True, 'Cuisine': True}\n"
     ]
    }
   ],
   "source": [
    "tag_dict = { }\n",
    "# Collect all unique tags via a dictionary\n",
    "for tags, sentence in train_sentences:\n",
    "    for tag in tags:\n",
    "        tag_dict[tag] = True\n",
    "        \n",
    "tags = []\n",
    "# Output all tags used\n",
    "for key in tag_dict:\n",
    "    print(key)\n",
    "    tags.append(key)\n",
    "print(tags)\n",
    "print(tag_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are 9 different NER tags being used. Note that the tag *O* means Other.   \n",
    "So when we look at the sentence below, we can see that each word of a sentence in our corpus contains a NER tag:  \n",
    "['5', 'star', 'resturants', 'in', 'my', 'town']      \n",
    "['Rating', 'Rating', 'O', 'Location', 'Location', 'Location']   \n",
    "\n",
    "For example '5' is tagged *Rating*, 'in' is tagged *Location*, 'town' is tagged *Location*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a tag and list of tag/sentence lists, returns the list of all unique words with that\n",
    "# NER tag.\n",
    "def tag_words(target_tag, sentences):\n",
    "    words = []\n",
    "    for tags, sentence in sentences:\n",
    "       # print(tags) = ['Rating', 'Rating', 'O','O', ...]\n",
    "        #print (sentences) = ['2', 'start', ...]\n",
    "        for tag, word in zip(tags, sentence):\n",
    "            #zip(tags, sentence) = {('Rating', '2'), ('Rating', 'start'), ...}\n",
    "            if (target_tag == tag) and (not word in words):\n",
    "                words.append(word)\n",
    "    words.sort()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q1 - 2 marks**   \n",
    "Using the function *tag_words()* above, explore the words associated with each type of named entity from *train_sentences*.  Write a few lines of code to output a few examples of *each type* of entity, as well as the number of different words for each type of entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: Rating \n",
      "First 30 Words: ['2', '3', '4', '5', 'a', 'at', 'best', 'bottle', 'busy', 'crowd', 'delicious', 'descent', 'excellent', 'famous', 'favorite', 'few', 'first', 'five', 'food', 'four', 'good', 'great', 'health', 'high', 'highest', 'highly', 'least', 'local', 'month', 'nice'] \n",
      "Total number of different words: 58 \n",
      "Entity: O \n",
      "First 30 Words: ['00', '1', '100', '2', '20', '3', '30', '4', '6', '8', '98', 'a', 'able', 'ablle', 'about', 'accept', 'address', 'adress', 'all', 'allow', 'allows', 'along', 'alot', 'also', 'although', 'am', 'an', 'and', 'anniversary', 'any'] \n",
      "Total number of different words: 367 \n",
      "Entity: Amenity \n",
      "First 30 Words: ['10', '12', '17', '50s', 'a', 'accept', 'accepts', 'access', 'accessible', 'alcohol', 'all', 'allday', 'allow', 'allowed', 'ambiance', 'american', 'an', 'are', 'area', 'arrangements', 'as', 'at', 'atmosphere', 'bands', 'bar', 'bars', 'bathrooms', 'beer', 'beverages', 'big'] \n",
      "Total number of different words: 292 \n",
      "Entity: Location \n",
      "First 30 Words: ['1', '10', '135', '15', '2', '25', '3', '4', '5', '52nd', '8', '86th', 'a', 'abington', 'airport', 'along', 'and', 'angeles', 'annapolis', 'anywhere', 'area', 'arent', 'arlington', 'around', 'ashland', 'at', 'atlanta', 'atlantic', 'ave', 'avenue'] \n",
      "Total number of different words: 244 \n",
      "Entity: Restaurant_Name \n",
      "First 30 Words: ['1790', '5', 'abuelos', 'and', 'anthonys', 'apple', 'applebees', 'arbys', 'aromas', 'asai', 'ashland', 'bakery', 'bamboo', 'bar', 'barrel', 'bbq', 'beanery', 'bees', 'bell', 'bella', 'belle', 'bellinis', 'bells', 'bertucci', 'bertuccis', 'beverly', 'biscuit', 'bisuteki', 'bj', 'black'] \n",
      "Total number of different words: 230 \n",
      "Entity: Price \n",
      "First 30 Words: ['00', '1', '10', '15', 'a', 'affordable', 'amazing', 'arent', 'average', 'cheap', 'cheapest', 'cheaply', 'cost', 'deal', 'deals', 'decent', 'dining', 'dollar', 'dollars', 'expensive', 'fair', 'fairly', 'fantastic', 'fine', 'five', 'for', 'free', 'good', 'great', 'high'] \n",
      "Total number of different words: 63 \n",
      "Entity: Hours \n",
      "First 30 Words: ['00', '1', '10', '11', '11pm', '12', '2', '24', '2am', '30', '4', '5', '6', '7', '700', '8', '9', 'a', 'after', 'all', 'am', 'around', 'at', 'before', 'breakfast', 'brunch', 'daily', 'day', 'days', 'dining'] \n",
      "Total number of different words: 71 \n",
      "Entity: Dish \n",
      "First 30 Words: ['and', 'appetizers', 'baba', 'bacon', 'bagel', 'bagels', 'bar', 'bbq', 'bean', 'beans', 'beef', 'beer', 'birthday', 'boardwalk', 'boca', 'bottles', 'bourek', 'bowl', 'bratwurst', 'bread', 'brisket', 'brownies', 'buffalo', 'burger', 'burgers', 'burrito', 'caesar', 'caeser', 'cakes', 'cheese'] \n",
      "Total number of different words: 165 \n",
      "Entity: Cuisine \n",
      "First 30 Words: ['afghan', 'african', 'american', 'asian', 'authentic', 'bakery', 'bar', 'barbecue', 'barbeque', 'bars', 'beanery', 'beer', 'bistro', 'brazilian', 'breakfast', 'brewery', 'brunch', 'burger', 'burrito', 'cafe', 'cafes', 'cajun', 'cali', 'chain', 'chilean', 'chinese', 'chowder', 'coffee', 'colombian', 'cooked'] \n",
      "Total number of different words: 130 \n"
     ]
    }
   ],
   "source": [
    "# Give some examples (words) for each type of entity along with the number of different words\n",
    "# tagged with each entity in train_sentences\n",
    "#you have to send the tag you want to check with the tag list \n",
    "\n",
    "for tag in tags:\n",
    "    entity_words =  tag_words(tag, train_sentences)\n",
    "    #print entity, print some (30) words for each entity, and the total number of unique words in each entity\n",
    "    print(\"Entity: {} \\nFirst 30 Words: {} \\nTotal number of different words: {} \".format(tag, entity_words[:30], len(entity_words))) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Using Gazetteers for NER**    \n",
    "Now we will explore using Gazetteers for NER. Specifically, we will be using Wikipedia's list of Cuisines to along with Edit Distance to tag any tokens as *Cuisine*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***3.1. Setting up the Gazetteer***    \n",
    "First thing to do is to setup the Gazetteer from Wikipedia's list of Cuisines (https://en.wikipedia.org/wiki/List_of_cuisines). We could scrape the webpage and extract these, but for the sake of keeping it simple the notebook includes the file *cuisine_gazetteer.txt* that we will use to load the Gazetteer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the Gazetteer\n",
    "gazetteer = []\n",
    "with open(\"cuisine_gazetteer.txt\") as f:\n",
    "    gazetteer = f.read().splitlines()\n",
    "\n",
    "# Since we are working with Single Words, we will only keep the first word \n",
    "# of each index in our Gazatteer.\n",
    "gazetteer = list(set([cuisine.split()[0].lower() for cuisine in gazetteer]))\n",
    "gazetteer.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ainu\n",
      "albanian\n",
      "andhra\n",
      "anglo-indian\n",
      "arab\n",
      "argentine\n",
      "armenian\n",
      "assyrian\n",
      "awadhi\n",
      "azerbaijani\n",
      "balochi\n",
      "bangladeshi\n",
      "belarusian\n",
      "bengali\n",
      "berber\n",
      "brazilian\n",
      "buddhist\n",
      "bulgarian\n",
      "cajun\n",
      "cantonese\n",
      "caribbean\n",
      "chechen\n",
      "chinese\n",
      "circassian\n",
      "crimean\n",
      "cypriot\n",
      "danish\n",
      "english\n",
      "estonian\n",
      "filipino\n",
      "french\n",
      "georgian\n",
      "german\n",
      "goan\n",
      "greek\n",
      "gujarati\n",
      "hong\n",
      "hyderabad\n",
      "indian\n",
      "indonesian\n",
      "inuit\n",
      "irish\n",
      "italian\n",
      "jamaican\n",
      "japanese\n",
      "jewish\n",
      "karnataka\n",
      "kazakh\n",
      "keralite\n",
      "korean\n",
      "kurdish\n",
      "laotian\n",
      "latvian\n",
      "lebanese\n",
      "lithuanian\n",
      "louisiana\n",
      "maharashtrian\n",
      "malay\n",
      "malaysian\n",
      "mangalorean\n",
      "mediterranean\n",
      "mexican\n",
      "mordovian\n",
      "mughal\n",
      "native\n",
      "nepalese\n",
      "new\n",
      "odia\n",
      "pakistani\n",
      "parsi\n",
      "pashtun\n",
      "pennsylvania\n",
      "peranakan\n",
      "persian\n",
      "peruvian\n",
      "polish\n",
      "portuguese\n",
      "punjabi\n",
      "rajasthani\n",
      "romanian\n",
      "russian\n",
      "sami\n",
      "serbian\n",
      "sindhi\n",
      "slovak\n",
      "slovenian\n",
      "somali\n",
      "south\n",
      "soviet\n",
      "spanish\n",
      "sri\n",
      "taiwanese\n",
      "tamil\n",
      "tatar\n",
      "thai\n",
      "turkish\n",
      "udupi\n",
      "ukrainian\n",
      "vietnamese\n",
      "yamal\n",
      "zambian\n",
      "zanzibari\n"
     ]
    }
   ],
   "source": [
    "# Look at the contents of the Gazetteer\n",
    "gazetteer\n",
    "for term in gazetteer:\n",
    "    print(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***3.2. Tagging Cuisines using Edit Distance***    \n",
    "Now that we have the Gazetteer, we need to define a metric to determine whether a token should be tagged as *Cuisine*. To do this we will use *Edit Distance*.   \n",
    "Recall that for regular edit distance we only look at *Deletion* (+1), *Insertion* (+1), and *Replacement* (+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the Edit Distance between two words s1 and s2\n",
    "def edit_distance(s1, s2):\n",
    "    m=len(s1)+1\n",
    "    n=len(s2)+1\n",
    "\n",
    "    tbl = {}\n",
    "    for i in range(m): tbl[i,0]=i\n",
    "    for j in range(n): tbl[0,j]=j\n",
    "    for i in range(1, m):\n",
    "        for j in range(1, n):\n",
    "            cost = 0 if s1[i-1] == s2[j-1] else 1\n",
    "            tbl[i,j] = min(tbl[i, j-1]+1, tbl[i-1, j]+1, tbl[i-1, j-1]+cost)\n",
    "\n",
    "    return tbl[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edit Distance between tuesday and thursday: 2\n",
      "Edit Distance between artificial and intelligence: 8\n",
      "Edit Distance between fresh and frozen: 4\n",
      "Edit Distance between within and lithuanian: 5\n"
     ]
    }
   ],
   "source": [
    "# Let us check the edit-distance with a few examples\n",
    "print(\"Edit Distance between tuesday and thursday:\", edit_distance(\"tuesday\", \"thursday\"))\n",
    "print(\"Edit Distance between artificial and intelligence:\", edit_distance(\"artificial\", \"thursday\"))\n",
    "print(\"Edit Distance between fresh and frozen:\", edit_distance(\"fresh\", \"frozen\"))\n",
    "print(\"Edit Distance between within and lithuanian:\", edit_distance(\"within\", \"lithuanian\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q2 - 2 marks**      \n",
    "With an understanding of the edit_distance function, we will define the function *ner_ed* that computes the edit distance between all tokens in our corpus with each item in our Gazetteer. If the edit distance between a token and a cuisine from our Gazetteer is under a specified threshold (an integer) we classify it as the same tag as those in our Gazetteer (in this case, *Cuisine*) by appending it to the list of words containing that tag. Specifically, we append the token and the gazetteer term as 'token/term'. The function returns a sorted list of the words that are determined to have the same tag as specified by the Gazetteer.    \n",
    "\n",
    "Below is a partially completed implementation of ner_ed which you must complete. Understanding the description of the function above you must complete the implementation below by adding the appropriate loop structure to allow the edit_distance logic to be correctly computed (already there, no need to edit it just need to setup the loops to be compatible). Recall that the structure of sentences is a list of lists ([[tags], [tokens]]). Also note that we will call a word from a Gazetteer a *term* and a word from a sentence a *token*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a list of tag/sentence lists, a gazetteer, and a threshold, compute the edit distance\n",
    "# between each token from each sentence to each cuisine from the gazatteer. If the edit distance\n",
    "# is <= a provided threshold, we collect the word as a word that would be tagged with the same tag\n",
    "# as the Gazetteer content.\n",
    "# In this section the tag would be cuisine since the gazetteer is based on cuisines.\n",
    "def ner_ed(sentences, gazetteer, threshold):\n",
    "    words = []\n",
    "    # TO DO - Setup the three loops:\n",
    "    # Loop 1\n",
    "    for tags, sentences in sentences:\n",
    "        #Loop 2 (introduces the variable token used below)\n",
    "        for token in sentences:\n",
    "            # Loop 3 (introduces the variable term used below)\n",
    "            for term in gazetteer:\n",
    "                # Computing the edit distance between sentence tokens and \n",
    "                # the Gazetteer terms.\n",
    "                if (edit_distance(token, term) <= threshold):\n",
    "                    if (not (token + \"/\" + term) in words):\n",
    "                        words.append(token + \"/\" + term)\n",
    "                    break\n",
    "        \n",
    "    # Sort the list\n",
    "    words.sort()\n",
    "    return words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this function, let's test it using three different thresholds on *test_sentences*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below will give sorted lists of words that tagged as Cuisine\n",
    "# This may take 40+ seconds (depending on your CPU), so be patient\n",
    "\n",
    "# Threshold = 0\n",
    "gz_ed_th0_words = ner_ed(test_sentences, gazetteer, threshold=0)\n",
    "# Threshold = 1\n",
    "gz_ed_th1_words = ner_ed(test_sentences, gazetteer, threshold=1)\n",
    "# Threshold = 2\n",
    "gz_ed_th2_words = ner_ed(test_sentences, gazetteer, threshold=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance 0\n",
      "['brazilian/brazilian', 'cajun/cajun', 'chinese/chinese', 'english/english', 'french/french', 'german/german', 'greek/greek', 'indian/indian', 'irish/irish', 'italian/italian', 'japanese/japanese', 'korean/korean', 'malaysian/malaysian', 'mexican/mexican', 'new/new', 'spanish/spanish', 'taiwanese/taiwanese', 'thai/thai', 'turkish/turkish', 'vietnamese/vietnamese']\n",
      "Distance 1\n",
      "['brazilian/brazilian', 'cajun/cajun', 'chinese/chinese', 'crab/arab', 'english/english', 'few/new', 'french/french', 'german/german', 'greek/greek', 'green/greek', 'indian/indian', 'irish/irish', 'italian/italian', 'japanese/japanese', 'korean/korean', 'long/hong', 'malaysian/malaysian', 'mexican/mexican', 'new/new', 'now/new', 'portugues/portuguese', 'spanish/spanish', 'taiwanese/taiwanese', 'thai/thai', 'than/thai', 'that/thai', 'tong/hong', 'turkish/turkish', 'vietnamese/vietnamese']\n",
      "Distance 2\n",
      "['along/hong', 'am/sami', 'american/mexican', 'an/ainu', 'and/ainu', 'any/ainu', 'are/arab', 'area/arab', 'atire/native', 'away/arab', 'barat/arab', 'bars/parsi', 'bean/goan', 'bec/new', 'beer/berber', 'both/south', 'brazilian/brazilian', 'brunch/french', 'burger/berber', 'cajun/cajun', 'can/cajun', 'caual/yamal', 'cheese/chinese', 'chicken/chechen', 'chinese/chinese', 'crab/arab', 'dim/odia', 'dine/ainu', 'dish/danish', 'dog/hong', 'down/goan', 'e/new', 'english/english', 'family/tamil', 'few/new', 'fin/ainu', 'find/ainu', 'fine/ainu', 'free/greek', 'french/french', 'fresh/french', 'fruit/inuit', 'fry/sri', 'ga/goan', 'german/german', 'get/new', 'go/goan', 'going/goan', 'good/goan', 'grat/arab', 'great/greek', 'greek/greek', 'green/greek', 'has/thai', 'hey/new', 'hi/sri', 'home/hong', 'hot/hong', 'hour/hong', 'how/hong', 'i/sri', 'idea/odia', 'in/ainu', 'indian/indian', 'irish/irish', 'italian/italian', 'japanese/japanese', 'john/goan', 'king/ainu', 'know/new', 'korean/korean', 'lamb/sami', 'latin/laotian', 'le/new', 'let/new', 'lone/hong', 'long/hong', 'malaysian/malaysian', 'mall/malay', 'may/malay', 'me/new', 'menu/ainu', 'mexican/mexican', 'morgan/korean', 'movie/soviet', 'nail/tamil', 'name/sami', 'near/new', 'neck/new', 'need/new', 'new/new', 'no/new', 'non/goan', 'north/south', 'not/new', 'now/new', 'ny/new', 'oak/goan', 'ohio/odia', 'oil/odia', 'on/goan', 'one/hong', 'or/sri', 'out/south', 'own/goan', 'park/parsi', 'per/new', 'pet/new', 'phone/hong', 'pine/ainu', 'portugues/portuguese', 'prix/sri', 'ranch/french', 'raw/arab', 'rd/sri', 'rib/arab', 'road/goan', 'route/south', 'salad/malay', 'sea/new', 'see/new', 'sin/ainu', 'small/somali', 'some/sami', 'soul/south', 'soup/south', 'spanish/danish', 'st/sri', 'star/tatar', 'sub/sri', 'sum/sami', 'taiwanese/taiwanese', 'tapas/tatar', 'ten/new', 'tgi/sri', 'th/thai', 'thai/thai', 'than/goan', 'that/thai', 'thats/thai', 'the/thai', 'their/thai', 'them/thai', 'they/thai', 'this/thai', 'thru/thai', 'tin/ainu', 'todai/thai', 'tong/hong', 'town/goan', 'try/sri', 'turkish/kurdish', 'vietnamese/vietnamese', 'view/new', 'what/thai', 'wine/ainu', 'wing/ainu', 'wrap/arab', 'yes/new', 'yet/new']\n"
     ]
    }
   ],
   "source": [
    "print(\"Distance 0\")\n",
    "print(gz_ed_th0_words)\n",
    "print(\"Distance 1\")\n",
    "print(gz_ed_th1_words)\n",
    "print(\"Distance 2\")\n",
    "print(gz_ed_th2_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q3 - 4 marks**   \n",
    "1)Describe what starts to happen as the threshold increases. Are any other cuisines found by increasing the threshold (That weren't previously there)?\n",
    "2) A good threshold should be relative to the length of the term. This means that rather than using an integer threshold, we use a float percentage. Then, when checking the threshold, we multiply the threshold by the length of the Gazetteer term. Copy over your implementation of the *ner_ed* function into the *ner_ed_rel* below, modify the function to use a relative threshold as described above. \n",
    "3) Test this new approach using a 25% and 50% threshold. Print the returned lists.    \n",
    "4) Does relative thresholding better capture variations than not using a relative thresholding? ***Look at the short words vs longer words.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3 - ANSWER (1)   \n",
    "<b> Describe what starts to happen as the threshold increases: </b>\n",
    " The number of tags increase as the threshold increases due to the fact that there is now leniency for acceptance for matches (with more tokens being able to match other cuisine with the now increased threshold for the insertion / deletion / replacement operations ). More variation is being captured. <br/>\n",
    "<b> Are any other cuisines found by increasing the thershold (that weren't previously there)? </b>   \n",
    "Yes. For example, portugues/portuguese can be found at threshold 1 & 2 and not in threshold 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3 - ANSWER (2)\n",
    "\n",
    "def ner_ed_rel(sentences, gazetteer, threshold_percent):\n",
    "    words = []\n",
    "    # TO DO - Setup the three loops:\n",
    "    # Loop 1\n",
    "    for tags, sentences in sentences:\n",
    "        #Loop 2 (introduces the variable token used below)\n",
    "        for token in sentences:\n",
    "            # Loop 3 (introduces the variable term used below)\n",
    "            for term in gazetteer:\n",
    "                # Computing the edit distance between sentence tokens and \n",
    "                # the Gazetteer terms.\n",
    "                # NEW - Calculate threshold based on the length of the terms \n",
    "                term_length = len(term)\n",
    "                if (edit_distance(token, term) <= (threshold_percent * term_length)):\n",
    "                    if (not (token + \"/\" + term) in words):\n",
    "                        words.append(token + \"/\" + term)\n",
    "                    break\n",
    "        \n",
    "    # Sort the list\n",
    "    words.sort()\n",
    "    return words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance 25%\n",
      "['brazilian/brazilian', 'cajun/cajun', 'chinese/chinese', 'crab/arab', 'english/english', 'french/french', 'german/german', 'greek/greek', 'green/greek', 'indian/indian', 'irish/irish', 'italian/italian', 'japanese/japanese', 'korean/korean', 'long/hong', 'malaysian/malaysian', 'mexican/mexican', 'new/new', 'portugues/portuguese', 'spanish/spanish', 'taiwanese/taiwanese', 'thai/thai', 'than/thai', 'that/thai', 'tong/hong', 'turkish/turkish', 'vietnamese/vietnamese']\n",
      "Distance 50%\n",
      "['afghan/mughal', 'alicias/albanian', 'along/hong', 'am/sami', 'american/armenian', 'an/ainu', 'and/ainu', 'anthonys/cantonese', 'any/ainu', 'anyone/cantonese', 'are/arab', 'area/arab', 'asian/albanian', 'atire/native', 'atlanta/albanian', 'attire/native', 'away/arab', 'barat/arab', 'bars/parsi', 'bean/german', 'beef/berber', 'been/berber', 'beer/berber', 'both/south', 'brazilian/belarusian', 'brunch/french', 'burger/berber', 'burgers/berber', 'cajun/cajun', 'cambodian/albanian', 'can/cajun', 'caribe/caribbean', 'caual/yamal', 'center/berber', 'changs/chinese', 'chase/chinese', 'checks/chechen', 'cheese/chechen', 'cheeses/chechen', 'chicken/chechen', 'china/chinese', 'chinese/cantonese', 'clean/crimean', 'crab/arab', 'cream/crimean', 'cuisine/louisiana', 'dance/danish', 'dancing/danish', 'danny/danish', 'danton/cantonese', 'date/native', 'deals/nepalese', 'dim/odia', 'dine/ainu', 'dining/danish', 'dish/danish', 'dog/hong', 'down/goan', 'drive/native', 'english/english', 'ethiopian/estonian', 'family/tamil', 'fancy/french', 'farmer/berber', 'few/new', 'fin/ainu', 'find/ainu', 'fine/ainu', 'five/native', 'formal/german', 'fredas/french', 'freds/french', 'free/french', 'french/french', 'fresh/french', 'fruit/inuit', 'fusion/russian', 'ga/goan', 'garden/german', 'gate/native', 'german/georgian', 'getting/argentine', 'give/native', 'go/goan', 'going/goan', 'good/goan', 'granit/keralite', 'grassa/circassian', 'grat/arab', 'great/german', 'greek/greek', 'green/german', 'has/thai', 'have/native', 'hawaiian/albanian', 'health/keralite', 'here/berber', 'hi/thai', 'himalayan/malaysian', 'home/hong', 'hot/hong', 'hour/hong', 'how/hong', 'idea/indian', 'iguana/lithuanian', 'in/ainu', 'indian/anglo-indian', 'inside/indian', 'irish/danish', 'italian/albanian', 'italos/italian', 'jaimes/japanese', 'japanese/cantonese', 'john/goan', 'joint/soviet', 'kendall/bengali', 'king/ainu', 'kitchen/chechen', 'korean/georgian', 'kostas/korean', 'lamb/sami', 'lane/lebanese', 'late/native', 'latin/albanian', 'least/lebanese', 'list/polish', 'live/native', 'loaves/lebanese', 'local/slovak', 'location/romanian', 'lone/hong', 'long/hong', 'look/slovak', 'lunch/french', 'malaysian/albanian', 'mall/malay', 'manhattan/mangalorean', 'may/malay', 'meal/mughal', 'menu/ainu', 'mexican/armenian', 'mission/circassian', 'more/korean', 'morgan/georgian', 'movie/soviet', 'nail/native', 'name/native', 'names/nepalese', 'nearest/lebanese', 'nears/nepalese', 'nearst/nepalese', 'new/new', 'nice/native', 'non/goan', 'north/south', 'now/new', 'number/berber', 'oak/goan', 'occasion/circassian', 'ohio/odia', 'oil/odia', 'olive/native', 'on/goan', 'one/hong', 'open/korean', 'order/berber', 'oriental/armenian', 'out/south', 'own/goan', 'park/parsi', 'pasta/pakistani', 'pasteur/pashtun', 'phone/hong', 'pine/ainu', 'please/lebanese', 'portion/georgian', 'portions/portuguese', 'portugues/portuguese', 'quality/keralite', 'ranch/danish', 'rating/native', 'raw/arab', 'review/soviet', 'rib/arab', 'road/goan', 'rolls/polish', 'romantic/romanian', 'route/south', 'salad/malay', 'seaver/berber', 'section/serbian', 'serve/berber', 'serves/berber', 'service/keralite', 'serving/peruvian', 'side/sindhi', 'sin/ainu', 'small/somali', 'some/sami', 'sonic/estonian', 'soul/somali', 'soup/south', 'spanish/danish', 'star/tatar', 'steak/slovak', 'street/soviet', 'sugar/mughal', 'sum/sami', 'sunday/sindhi', 'sushi/sindhi', 'taiwanese/cantonese', 'tapas/japanese', 'tgi/thai', 'th/thai', 'thai/thai', 'than/goan', 'that/thai', 'thats/thai', 'the/thai', 'their/thai', 'them/thai', 'they/thai', 'this/thai', 'thru/thai', 'time/native', 'tin/ainu', 'todai/somali', 'tomato/somali', 'tong/hong', 'town/goan', 'trendy/french', 'turkish/kurdish', 'under/andhra', 'vegan/georgian', 'vietnamese/vietnamese', 'view/soviet', 'virgin/georgian', 'what/thai', 'wine/ainu', 'wing/ainu', 'with/jewish', 'within/lithuanian', 'wrap/arab']\n"
     ]
    }
   ],
   "source": [
    "# Q3 - ANSWER (3)\n",
    "# 25% Threshold\n",
    "gz_ed_25_words = ner_ed_rel(test_sentences, gazetteer, threshold_percent=0.25)\n",
    "# 50% threshold\n",
    "gz_ed_50_words = ner_ed_rel(test_sentences, gazetteer, threshold_percent=0.5)\n",
    "print(\"Distance 25%\")\n",
    "print(gz_ed_25_words)\n",
    "print(\"Distance 50%\")\n",
    "print(gz_ed_50_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3 - ANSWER (4)    \n",
    "Does relative thresholding better capture variations than not using a relative thresholding?   \n",
    "Yes, relative thresholding allows for more variations to be captured. In a normal threshold, if the threshold is 1 then the length of the terms is not taken into account and is, therefore, more restrictive. The higher the percentage, the more variation is captured in the relative thresholding.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Exploring Wordnet**   \n",
    "Let's first explore a bit the wordnet interface within nltk.  \n",
    "You can also look a the [WordNet interface description](http://www.nltk.org/howto/wordnet.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('evaluation.n.02'), Synset('evaluation.n.01'), Synset('rating.n.03'), Synset('military_rank.n.01'), Synset('rate.v.01'), Synset('rate.v.02'), Synset('rate.v.03'), Synset('rat.v.01'), Synset('rat.v.02'), Synset('fink.v.01'), Synset('rat.v.04'), Synset('rat.v.05'), Synset('denounce.v.04')]\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "# A synset is a concept associated with a set of synonyms\n",
    "ratingSenses = wordnet.synsets('rating')\n",
    "print(ratingSenses)\n",
    "print(len(ratingSenses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that there are 13 senses of rating, 4 nouns and 9 verbs.  The word displayed is the most representative word for each sense.  \n",
    "\n",
    "You can try other words.  I recommend that you also perform the same search [online](http://wordnetweb.princeton.edu/perl/webwn) to better understand the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the basic information in each synset.        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a function to print the basic information\n",
    "def printBasicSynsetInfo(d):\n",
    "    print(\"SynLemmas\")\n",
    "    print(d.lemmas())\n",
    "    # Print the synonyms\n",
    "    print(\"Synonyms\")\n",
    "    synonyms = [l.name() for l in d.lemmas()]\n",
    "    print(synonyms)\n",
    "    # Print the definition\n",
    "    print(\"Definition\")\n",
    "    print(d.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sense 0]\n",
      "SynLemmas\n",
      "[Lemma('evaluation.n.02.evaluation'), Lemma('evaluation.n.02.valuation'), Lemma('evaluation.n.02.rating')]\n",
      "Synonyms\n",
      "['evaluation', 'valuation', 'rating']\n",
      "Definition\n",
      "an appraisal of the value of something\n",
      "\n",
      "[Sense 1]\n",
      "SynLemmas\n",
      "[Lemma('evaluation.n.01.evaluation'), Lemma('evaluation.n.01.rating')]\n",
      "Synonyms\n",
      "['evaluation', 'rating']\n",
      "Definition\n",
      "act of ascertaining or fixing the value or worth of\n",
      "\n",
      "[Sense 2]\n",
      "SynLemmas\n",
      "[Lemma('rating.n.03.rating')]\n",
      "Synonyms\n",
      "['rating']\n",
      "Definition\n",
      "standing or position on a scale\n",
      "\n",
      "[Sense 3]\n",
      "SynLemmas\n",
      "[Lemma('military_rank.n.01.military_rank'), Lemma('military_rank.n.01.military_rating'), Lemma('military_rank.n.01.paygrade'), Lemma('military_rank.n.01.rating')]\n",
      "Synonyms\n",
      "['military_rank', 'military_rating', 'paygrade', 'rating']\n",
      "Definition\n",
      "rank in a military organization\n",
      "\n",
      "[Sense 4]\n",
      "SynLemmas\n",
      "[Lemma('rate.v.01.rate'), Lemma('rate.v.01.rank'), Lemma('rate.v.01.range'), Lemma('rate.v.01.order'), Lemma('rate.v.01.grade'), Lemma('rate.v.01.place')]\n",
      "Synonyms\n",
      "['rate', 'rank', 'range', 'order', 'grade', 'place']\n",
      "Definition\n",
      "assign a rank or rating to\n",
      "\n",
      "[Sense 5]\n",
      "SynLemmas\n",
      "[Lemma('rate.v.02.rate')]\n",
      "Synonyms\n",
      "['rate']\n",
      "Definition\n",
      "be worthy of or have a certain rating\n",
      "\n",
      "[Sense 6]\n",
      "SynLemmas\n",
      "[Lemma('rate.v.03.rate'), Lemma('rate.v.03.value')]\n",
      "Synonyms\n",
      "['rate', 'value']\n",
      "Definition\n",
      "estimate the value of\n",
      "\n",
      "[Sense 7]\n",
      "SynLemmas\n",
      "[Lemma('rat.v.01.rat')]\n",
      "Synonyms\n",
      "['rat']\n",
      "Definition\n",
      "desert one's party or group of friends, for example, for one's personal advantage\n",
      "\n",
      "[Sense 8]\n",
      "SynLemmas\n",
      "[Lemma('rat.v.02.rat')]\n",
      "Synonyms\n",
      "['rat']\n",
      "Definition\n",
      "employ scabs or strike breakers in\n",
      "\n",
      "[Sense 9]\n",
      "SynLemmas\n",
      "[Lemma('fink.v.01.fink'), Lemma('fink.v.01.scab'), Lemma('fink.v.01.rat'), Lemma('fink.v.01.blackleg')]\n",
      "Synonyms\n",
      "['fink', 'scab', 'rat', 'blackleg']\n",
      "Definition\n",
      "take the place of work of someone on strike\n",
      "\n",
      "[Sense 10]\n",
      "SynLemmas\n",
      "[Lemma('rat.v.04.rat')]\n",
      "Synonyms\n",
      "['rat']\n",
      "Definition\n",
      "give (hair) the appearance of being fuller by using a rat\n",
      "\n",
      "[Sense 11]\n",
      "SynLemmas\n",
      "[Lemma('rat.v.05.rat')]\n",
      "Synonyms\n",
      "['rat']\n",
      "Definition\n",
      "catch rats, especially with dogs\n",
      "\n",
      "[Sense 12]\n",
      "SynLemmas\n",
      "[Lemma('denounce.v.04.denounce'), Lemma('denounce.v.04.tell_on'), Lemma('denounce.v.04.betray'), Lemma('denounce.v.04.give_away'), Lemma('denounce.v.04.rat'), Lemma('denounce.v.04.grass'), Lemma('denounce.v.04.shit'), Lemma('denounce.v.04.shop'), Lemma('denounce.v.04.snitch'), Lemma('denounce.v.04.stag')]\n",
      "Synonyms\n",
      "['denounce', 'tell_on', 'betray', 'give_away', 'rat', 'grass', 'shit', 'shop', 'snitch', 'stag']\n",
      "Definition\n",
      "give away information about somebody\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can print the information for each sense of \"rating\"\n",
    "for i in range(len(ratingSenses)):\n",
    "    print(\"[Sense \" + str(i) + \"]\")\n",
    "    printBasicSynsetInfo(ratingSenses[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A rich taxonomy has been manually developed in Wordnet, making it a rich resource.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a function to print the basic information, receives a synset\n",
    "def printTaxonomyInfo(d):\n",
    "    # Print the synonmyms\n",
    "    synonyms = [l.name() for l in d.lemmas()]\n",
    "    print(\"Synonyms:\")\n",
    "    print(synonyms)\n",
    "    # Print the hypernyms\n",
    "    print(\"Hypernyms:\")\n",
    "    print(d.hypernyms())\n",
    "    # Print the hyponyms *** We will use these later so note how we get the hyponyms ***\n",
    "    print(\"Hyponyms:\")\n",
    "    print(d.hyponyms())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q4 - 2 marks**   \n",
    "Choose two words and write code to print the taxonomic information for all senses of those words using printTaxonomyInfo()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonyms:\n",
      "['delivery', 'bringing']\n",
      "Hypernyms:\n",
      "[Synset('transportation.n.02')]\n",
      "Hyponyms:\n",
      "[Synset('airdrop.n.01'), Synset('consignment.n.03'), Synset('passage.n.10'), Synset('post.n.11'), Synset('service.n.13')]\n",
      "Synonyms:\n",
      "['delivery']\n",
      "Hypernyms:\n",
      "[Synset('birth.n.02')]\n",
      "Hyponyms:\n",
      "[]\n",
      "Synonyms:\n",
      "['manner_of_speaking', 'speech', 'delivery']\n",
      "Hypernyms:\n",
      "[Synset('expressive_style.n.01')]\n",
      "Hyponyms:\n",
      "[Synset('address.n.04'), Synset('catch.n.05'), Synset('elocution.n.01'), Synset('modulation.n.04'), Synset('prosody.n.01'), Synset('shibboleth.n.02'), Synset('tone.n.01'), Synset('tongue.n.04')]\n",
      "Synonyms:\n",
      "['delivery', 'livery', 'legal_transfer']\n",
      "Hypernyms:\n",
      "[Synset('conveyance.n.04')]\n",
      "Hyponyms:\n",
      "[Synset('bailment.n.01'), Synset('surrender.n.03')]\n",
      "Synonyms:\n",
      "['pitch', 'delivery']\n",
      "Hypernyms:\n",
      "[Synset('throw.n.01')]\n",
      "Hyponyms:\n",
      "[Synset('balk.n.04'), Synset('ball.n.12'), Synset('beanball.n.01'), Synset('change-up.n.01'), Synset('curve.n.03'), Synset('duster.n.04'), Synset('fastball.n.01'), Synset('knuckleball.n.01'), Synset('overhand_pitch.n.01'), Synset('passed_ball.n.01'), Synset('screwball.n.02'), Synset('sinker.n.03'), Synset('spitball.n.02'), Synset('strike.n.05'), Synset('submarine_ball.n.01'), Synset('wild_pitch.n.01')]\n",
      "Synonyms:\n",
      "['rescue', 'deliverance', 'delivery', 'saving']\n",
      "Hypernyms:\n",
      "[Synset('recovery.n.03')]\n",
      "Hyponyms:\n",
      "[Synset('lifesaving.n.01'), Synset('reclamation.n.02'), Synset('redemption.n.01'), Synset('salvage.n.02'), Synset('salvage.n.03'), Synset('salvation.n.04'), Synset('search_and_rescue_mission.n.01')]\n",
      "Synonyms:\n",
      "['delivery', 'obstetrical_delivery']\n",
      "Hypernyms:\n",
      "[Synset('act.n.02')]\n",
      "Hyponyms:\n",
      "[Synset('breech_delivery.n.01'), Synset('cesarean_delivery.n.01'), Synset('forceps_delivery.n.01'), Synset('midwifery.n.02')]\n",
      "Synonyms:\n",
      "['send', 'direct']\n",
      "Hypernyms:\n",
      "[Synset('move.v.02')]\n",
      "Hyponyms:\n",
      "[Synset('blow.v.13'), Synset('divert.v.02'), Synset('project.v.10'), Synset('redirect.v.01'), Synset('refer.v.04'), Synset('route.v.02'), Synset('turn.v.11'), Synset('turn.v.20')]\n",
      "Synonyms:\n",
      "['send', 'send_out']\n",
      "Hypernyms:\n",
      "[Synset('transmit.v.04')]\n",
      "Hyponyms:\n",
      "[Synset('mail.v.01'), Synset('mail_out.v.01'), Synset('send_in.v.01')]\n",
      "Synonyms:\n",
      "['mail', 'post', 'send']\n",
      "Hypernyms:\n",
      "[Synset('transfer.v.02')]\n",
      "Hyponyms:\n",
      "[Synset('airmail.v.01'), Synset('express-mail.v.01'), Synset('express.v.07'), Synset('register.v.09')]\n",
      "Synonyms:\n",
      "['transport', 'send', 'ship']\n",
      "Hypernyms:\n",
      "[Synset('move.v.02')]\n",
      "Hyponyms:\n",
      "[Synset('barge.v.02'), Synset('dispatch.v.01'), Synset('forward.v.01'), Synset('railroad.v.03')]\n",
      "Synonyms:\n",
      "['station', 'post', 'send', 'place']\n",
      "Hypernyms:\n",
      "[Synset('move.v.02')]\n",
      "Hyponyms:\n",
      "[Synset('fort.v.03'), Synset('garrison.v.01'), Synset('locate.v.03')]\n",
      "Synonyms:\n",
      "['send', 'get_off', 'send_off']\n",
      "Hypernyms:\n",
      "[Synset('transfer.v.02')]\n",
      "Hyponyms:\n",
      "[]\n",
      "Synonyms:\n",
      "['commit', 'institutionalize', 'institutionalise', 'send', 'charge']\n",
      "Hypernyms:\n",
      "[Synset('transfer.v.02')]\n",
      "Hyponyms:\n",
      "[Synset('hospitalize.v.01')]\n",
      "Synonyms:\n",
      "['air', 'send', 'broadcast', 'beam', 'transmit']\n",
      "Hypernyms:\n",
      "[Synset('publicize.v.01')]\n",
      "Hyponyms:\n",
      "[Synset('interrogate.v.01'), Synset('rerun.v.01'), Synset('satellite.v.01'), Synset('sportscast.v.01'), Synset('telecast.v.01')]\n"
     ]
    }
   ],
   "source": [
    "# TO DO - Choose two words and print the taxonomix information for all \n",
    "# senses of those words using printTaxonomyInfo\n",
    "\n",
    "# Word 1\n",
    "#get senses \n",
    "deliverySynsets = wordnet.synsets('delivery')\n",
    "#get taxonomic info for all senses of the words \n",
    "for synset in deliverySynsets:\n",
    "    printTaxonomyInfo(synset)\n",
    "    \n",
    "# Word 2\n",
    "sendSynsets = wordnet.synsets('send')\n",
    "for synset in sendSynsets:\n",
    "    printTaxonomyInfo(synset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to show how to get the names from each hyponym by using our example *ratingSenses*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sense 0]\n",
      "bond_rating\n",
      "mark\n",
      "overvaluation\n",
      "pricing\n",
      "reevaluation\n",
      "undervaluation\n",
      "\n",
      "[Sense 1]\n",
      "marking\n",
      "\n",
      "[Sense 2]\n",
      "\n",
      "[Sense 3]\n",
      "flag_rank\n",
      "\n",
      "[Sense 4]\n",
      "downgrade\n",
      "prioritize\n",
      "reorder\n",
      "seed\n",
      "sequence\n",
      "shortlist\n",
      "subordinate\n",
      "superordinate\n",
      "upgrade\n",
      "\n",
      "[Sense 5]\n",
      "\n",
      "[Sense 6]\n",
      "revalue\n",
      "\n",
      "[Sense 7]\n",
      "\n",
      "[Sense 8]\n",
      "\n",
      "[Sense 9]\n",
      "\n",
      "[Sense 10]\n",
      "\n",
      "[Sense 11]\n",
      "\n",
      "[Sense 12]\n",
      "sell_out\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(ratingSenses)):\n",
    "    print(\"[Sense \" +  str(i) + \"]\")\n",
    "    for hyponym in ratingSenses[i].hyponyms():\n",
    "        # Access the hyponym names with: hyponym.name().split(\".\")[0]\n",
    "        print(hyponym.name().split(\".\")[0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Using Wordnet for NER**   \n",
    "Having explored Wordnet, we will use what we have learned to try another method of NER tagging. Specifically, we will be trying to tag any words labelled *Dish* by using the edit distance and the edit distance thresholding technique that we used in section 3 with the *hyponyms* for the word *dish*.   \n",
    "  \n",
    "Since we have covered everything needed to answer this question in this notebook, you will be doing most of the coding yourself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(TO DO) Q5 - 5 marks**   \n",
    "1) Go through the senses for the word '*dish*' (recall how we used the functions printBasicSynsetInfo() and wordnet.synsets() above). Looking through the senses, select the single most appropriate sense of the word *dish* for our problem (Restaurant Corpus, look into the corpus or file if you need more context on which words are labelled *Dish*). Collect a list of all *hyponyms* for the word '*dish*' based on the selected sense of the word. You can think of this as a Gazetteer.    \n",
    "2) Using thresholds 0, 1, and 2 with ner_ed, collect three  lists of words that we would tag as *Dish* based on the *Edit Distance* between our Dish Gazetteer and test_sentences. Print the returned lists.     \n",
    "3) Are more dishes being found with a larger threshold? Give an example to justify your answer.    \n",
    "4) Using thresholds 25% and 50% with ner_ed_rel, collect two lists of words that we would tag as *Dish* based on the *Edit Distance* between our Dish Gazetteer and test_sentences. Print the returned lists.    \n",
    "5) Does relative thresholding better capture variations than not using a relative thresholding for the tag *Dish*? Give an example to justify your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sense 0]\n",
      "SynLemmas\n",
      "[Lemma('dish.n.01.dish')]\n",
      "Synonyms\n",
      "['dish']\n",
      "Definition\n",
      "a piece of dishware normally used as a container for holding or serving food\n",
      "\n",
      "[Sense 1]\n",
      "SynLemmas\n",
      "[Lemma('dish.n.02.dish')]\n",
      "Synonyms\n",
      "['dish']\n",
      "Definition\n",
      "a particular item of prepared food\n",
      "\n",
      "[Sense 2]\n",
      "SynLemmas\n",
      "[Lemma('dish.n.03.dish'), Lemma('dish.n.03.dishful')]\n",
      "Synonyms\n",
      "['dish', 'dishful']\n",
      "Definition\n",
      "the quantity that a dish will hold\n",
      "\n",
      "[Sense 3]\n",
      "SynLemmas\n",
      "[Lemma('smasher.n.02.smasher'), Lemma('smasher.n.02.stunner'), Lemma('smasher.n.02.knockout'), Lemma('smasher.n.02.beauty'), Lemma('smasher.n.02.ravisher'), Lemma('smasher.n.02.sweetheart'), Lemma('smasher.n.02.peach'), Lemma('smasher.n.02.lulu'), Lemma('smasher.n.02.looker'), Lemma('smasher.n.02.mantrap'), Lemma('smasher.n.02.dish')]\n",
      "Synonyms\n",
      "['smasher', 'stunner', 'knockout', 'beauty', 'ravisher', 'sweetheart', 'peach', 'lulu', 'looker', 'mantrap', 'dish']\n",
      "Definition\n",
      "a very attractive or seductive looking woman\n",
      "\n",
      "[Sense 4]\n",
      "SynLemmas\n",
      "[Lemma('dish.n.05.dish'), Lemma('dish.n.05.dish_aerial'), Lemma('dish.n.05.dish_antenna'), Lemma('dish.n.05.saucer')]\n",
      "Synonyms\n",
      "['dish', 'dish_aerial', 'dish_antenna', 'saucer']\n",
      "Definition\n",
      "directional antenna consisting of a parabolic reflector for microwave or radio frequency radiation\n",
      "\n",
      "[Sense 5]\n",
      "SynLemmas\n",
      "[Lemma('cup_of_tea.n.01.cup_of_tea'), Lemma('cup_of_tea.n.01.bag'), Lemma('cup_of_tea.n.01.dish')]\n",
      "Synonyms\n",
      "['cup_of_tea', 'bag', 'dish']\n",
      "Definition\n",
      "an activity that you like or at which you are superior\n",
      "\n",
      "[Sense 6]\n",
      "SynLemmas\n",
      "[Lemma('serve.v.06.serve'), Lemma('serve.v.06.serve_up'), Lemma('serve.v.06.dish_out'), Lemma('serve.v.06.dish_up'), Lemma('serve.v.06.dish')]\n",
      "Synonyms\n",
      "['serve', 'serve_up', 'dish_out', 'dish_up', 'dish']\n",
      "Definition\n",
      "provide (usually but not necessarily food)\n",
      "\n",
      "[Sense 7]\n",
      "SynLemmas\n",
      "[Lemma('dish.v.02.dish')]\n",
      "Synonyms\n",
      "['dish']\n",
      "Definition\n",
      "make concave; shape like a dish\n",
      "\n",
      "Sense 1\n",
      "['adobo', 'applesauce', 'bacon_and_eggs', 'barbecue', 'barbecued_spareribs', 'barbecued_wing', 'beef_bourguignonne', 'beef_stroganoff', 'beef_wellington', 'biryani', 'bitok', 'boiled_dinner', 'boiled_egg', 'boston_baked_beans', 'bubble_and_squeak', 'buffalo_wing', 'burrito', 'carbonnade_flamande', 'casserole', 'chicken_and_rice', 'chicken_cordon_bleu', 'chicken_kiev', 'chicken_marengo', 'chicken_paprika', 'chicken_provencale', 'chili', 'chop_suey', 'chow_mein', 'coq_au_vin', 'coquille', 'coquilles_saint-jacques', 'cottage_pie', 'couscous', 'croquette', 'curry', 'custard', 'deviled_egg', 'dolmas', 'egg_roll', 'eggs_benedict', 'enchilada', 'escalope_de_veau_orloff', 'falafel', 'fish_and_chips', 'fish_stick', 'fondue', 'fondue', 'french_toast', 'fried_egg', 'fried_rice', 'frittata', 'frog_legs', 'galantine', 'gefilte_fish', 'haggis', 'ham_and_eggs', 'hash', 'jambalaya', 'kabob', 'kedgeree', 'kishke', 'lobster_thermidor', 'lutefisk', 'macedoine', 'maryland_chicken', 'meat_loaf', 'meatball', 'mold', 'moo_goo_gai_pan', 'moussaka', 'mousse', 'omelet', 'osso_buco', 'paella', 'pasta', 'patty', 'pepper_steak', 'pheasant_under_glass', 'piece_de_resistance', 'pilaf', 'pizza', 'poached_egg', 'poi', 'pork_and_beans', 'porridge', 'potpie', 'pudding', 'ramekin', 'refried_beans', 'rijsttaffel', 'risotto', 'rissole', 'roulade', 'salad', 'salisbury_steak', 'sandwich_plate', 'sashimi', 'sauerbraten', 'sauerkraut', 'saute', 'scallopine', 'scampi', 'schnitzel', 'scotch_egg', 'scotch_woodcock', 'scrambled_eggs', 'scrapple', 'seafood_newburg', 'shirred_egg', 'side_dish', 'snack_food', 'souffle', 'soup', 'spaghetti_and_meatballs', 'spanish_rice', 'special', 'steak_au_poivre', 'steak_tartare', 'stew', 'stuffed_cabbage', 'stuffed_peppers', 'stuffed_tomato', 'stuffed_tomato', 'succotash', 'sukiyaki', 'sushi', 'swiss_steak', 'taco', 'tamale', 'tamale_pie', 'tempura', 'teriyaki', 'terrine', 'tetrazzini', 'timbale', 'tostada', 'turnover', 'veal_cordon_bleu', 'veal_parmesan', 'viand', 'welsh_rarebit']\n"
     ]
    }
   ],
   "source": [
    "# TO DO (1)\n",
    "# Look through the senses of the word dish to select the most appropriate sense for our corpus\n",
    "# Recall how we used the printBasicSynsetInfo() function above\n",
    "dishSenses = wordnet.synsets('dish')\n",
    "for i in range(len(dishSenses)):\n",
    "    print(\"[Sense \" + str(i) + \"]\")\n",
    "    printBasicSynsetInfo(dishSenses[i])\n",
    "    print()\n",
    "# Print which sense you have selected (ex: Sense X)\n",
    "print(\"Sense 1\")\n",
    "# Collect a list of all hyponyms for the word 'dish' only from the selected sense\n",
    "dish_hyponyms = []\n",
    "for hyponym in dishSenses[1].hyponyms():\n",
    "    dish_hyponyms.append(hyponym.name().split(\".\")[0])\n",
    "# Print the Dish Gazetteer\n",
    "print(dish_hyponyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['barbecue/barbecue', 'burrito/burrito', 'pasta/pasta', 'pizza/pizza', 'salad/salad', 'soup/soup', 'special/special', 'sushi/sushi', 'taco/taco']\n",
      "['barbecue/barbecue', 'barbeque/barbecue', 'burrito/burrito', 'carry/curry', 'child/chili', 'has/hash', 'meatballs/meatball', 'pasta/pasta', 'pizza/pizza', 'salad/salad', 'sauce/saute', 'soul/soup', 'soup/soup', 'special/special', 'specials/special', 'sushi/sushi', 'taco/taco', 'tacos/taco']\n",
      "['and/viand', 'as/hash', 'back/taco', 'barbecue/barbecue', 'barbeque/barbecue', 'box/poi', 'burrito/burrito', 'carry/curry', 'casa/hash', 'chase/hash', 'chick/chili', 'child/chili', 'china/chili', 'could/mold', 'course/mousse', 'curb/curry', 'date/saute', 'dish/hash', 'do/poi', 'dog/poi', 'dollars/dolmas', 'east/hash', 'fast/hash', 'few/stew', 'find/viand', 'food/mold', 'for/poi', 'four/soup', 'gate/saute', 'go/poi', 'good/mold', 'hard/hash', 'has/hash', 'have/hash', 'hi/poi', 'high/hash', 'hot/poi', 'hour/soup', 'house/mousse', 'how/poi', 'i/poi', 'jack/taco', 'late/saute', 'los/poi', 'maid/mold', 'mall/mold', 'meatballs/meatball', 'mid/mold', 'mile/mold', 'milk/mold', 'more/mold', 'most/mold', 'new/stew', 'no/poi', 'non/poi', 'not/poi', 'now/poi', 'of/poi', 'oil/poi', 'on/poi', 'or/poi', 'our/soup', 'out/soup', 'pasta/pasta', 'per/poi', 'pesto/pasta', 'pet/poi', 'pf/poi', 'pho/poi', 'pie/poi', 'pizza/pizza', 'pm/poi', 'pork/poi', 'port/poi', 'prix/poi', 'pub/poi', 'road/mold', 'route/saute', 'salad/salad', 'salon/salad', 'sauce/saute', 'sea/stew', 'see/stew', 'shop/soup', 'site/saute', 'some/soup', 'soul/soup', 'soup/soup', 'special/special', 'specials/special', 'spot/poi', 'st/stew', 'stand/viand', 'star/stew', 'state/saute', 'steak/stew', 'stop/soup', 'sub/soup', 'sum/soup', 'sushi/sushi', 'table/tamale', 'taco/taco', 'tacos/taco', 'take/taco', 'tameles/tamale', 'tasty/pasta', 'ten/stew', 'tgi/poi', 'to/poi', 'too/poi', 'two/taco', 'up/soup', 'view/stew', 'wild/mold', 'wok/poi', 'wood/mold', 'would/mold', 'you/poi', 'zoo/poi']\n"
     ]
    }
   ],
   "source": [
    "# TO DO (2)\n",
    "# Using thresholds 0, 1, and 2 with ner_ed, collect three lists of words that we\n",
    "# would tag as *Dish* based on the *Edit Distance* between our Dish Gazetteer and test_sentences. \n",
    "# Threshold = 0\n",
    "gz_dish_th0_words = ner_ed(test_sentences, dish_hyponyms, threshold=0)\n",
    "# Threshold = 1\n",
    "gz_dish_th1_words = ner_ed(test_sentences, dish_hyponyms, threshold=1)\n",
    "# Threshold = 2\n",
    "gz_dish_th2_words = ner_ed(test_sentences, dish_hyponyms, threshold=2)\n",
    "#Print the returned lists. \n",
    "print(\"Distance 0\")\n",
    "print(gz_dish_th0_words)\n",
    "print(\"Distance 1\")\n",
    "print(gz_dish_th1_words)\n",
    "print(\"Distance 2\")\n",
    "print(gz_dish_th2_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5 - TO DO (3)   \n",
    "Are more dishes being found with a larger threshold? Give an example to justify your answer.  \n",
    "Yes. meatballs/meatball, barbeque/barbecue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['barbecue/barbecue', 'barbeque/barbecue', 'burrito/burrito', 'carry/curry', 'child/chili', 'has/hash', 'meatballs/meatball', 'pasta/pasta', 'pizza/pizza', 'salad/salad', 'sauce/saute', 'soul/soup', 'soup/soup', 'special/special', 'specials/special', 'sushi/sushi', 'taco/taco', 'tacos/taco']\n",
      "['all/paella', 'and/viand', 'apple/applesauce', 'applebeas/applesauce', 'applebees/applesauce', 'as/hash', 'avenue/barbecue', 'back/taco', 'bagels/haggis', 'barbecue/barbecue', 'barbeque/barbecue', 'barrells/barbecue', 'bell/paella', 'buffalo/buffalo_wing', 'burger/turnover', 'burmese/barbecue', 'burrito/burrito', 'cactus/couscous', 'call/paella', 'card/custard', 'carry/curry', 'casa/hash', 'caual/tamale', 'central/meatball', 'changs/haggis', 'chase/hash', 'chick/chili', 'chicken/chicken_kiev', 'child/chili', 'china/chili', 'chowder/chow_mein', 'code/fondue', 'coffee/souffle', 'comella/coquille', 'cooks/couscous', 'could/coquille', 'coupons/couscous', 'course/coquille', 'create/croquette', 'curb/curry', 'date/saute', 'discover/turnover', 'dish/hash', 'dishes/kishke', 'does/dolmas', 'dollars/dolmas', 'donut/fondue', 'donuts/dolmas', 'during/pudding', 'east/hash', 'elmos/dolmas', 'family/tamale', 'fast/hash', 'few/stew', 'find/fondue', 'fine/fondue', 'food/fondue', 'formal/dolmas', 'four/fondue', 'franchises/french_toast', 'french/french_toast', 'frequent/croquette', 'fridays/frittata', 'fried/fried_egg', 'gate/saute', 'good/mold', 'grill/egg_roll', 'gustovs/couscous', 'haight/haggis', 'hard/hash', 'has/haggis', 'have/hash', 'hello/paella', 'high/hash', 'himalayan/jambalaya', 'hour/soup', 'hours/mousse', 'house/mousse', 'houses/couscous', 'jack/taco', 'kendall/meatball', 'kostas/dolmas', 'late/saute', 'latin/galantine', 'let/omelet', 'like/kishke', 'lone/fondue', 'made/tamale', 'maid/mold', 'make/tamale', 'mall/meatball', 'meal/meatball', 'meat/meatball', 'meatballs/meatball', 'meats/meatball', 'mid/mold', 'mike/kishke', 'mile/mold', 'miles/omelet', 'milk/mold', 'monacos/couscous', 'more/mold', 'morning/porridge', 'most/mold', 'movie/mousse', 'music/mousse', 'name/tamale', 'new/stew', 'noodle/fondue', 'nossa/moussaka', 'one/fondue', 'osaka/moussaka', 'our/soup', 'out/soup', 'parking/porridge', 'pasta/paella', 'people/potpie', 'pesto/pasta', 'pie/potpie', 'pizza/pizza', 'place/applesauce', 'please/applesauce', 'portion/porridge', 'portions/porridge', 'price/porridge', 'pricing/pudding', 'private/frittata', 'purple/potpie', 'require/coquille', 'river/turnover', 'road/mold', 'rolls/dolmas', 'rooms/dolmas', 'route/croquette', 'salad/salad', 'salon/salad', 'saloon/scallopine', 'sandwich/sandwich_plate', 'sauce/applesauce', 'sea/stew', 'seafood/snack_food', 'see/stew', 'sell/paella', 'sells/paella', 'service/terrine', 'serving/terrine', 'shake/kishke', 'shop/soup', 'site/saute', 'small/tamale', 'some/soup', 'soul/souffle', 'soup/soup', 'spanish/spanish_rice', 'special/special', 'specials/special', 'st/stew', 'stand/custard', 'star/custard', 'stars/custard', 'state/saute', 'steak/stew', 'stop/soup', 'sub/soup', 'sum/soup', 'sushi/sashimi', 'table/tamale', 'taco/taco', 'tacos/taco', 'take/taco', 'tameles/omelet', 'tapas/tamale', 'tasty/pasta', 'tell/paella', 'ten/stew', 'time/tamale', 'to/taco', 'tomato/tamale', 'too/taco', 'travel/turnover', 'two/taco', 'under/turnover', 'up/soup', 'use/mousse', 'valet/omelet', 'view/stew', 'villa/paella', 'waffle/souffle', 'waterfront/sauerkraut', 'well/paella', 'wild/mold', 'wood/mold', 'would/mold', 'you/soup']\n"
     ]
    }
   ],
   "source": [
    "# TO DO (4)\n",
    "# Using thresholds 25% and 50% with ner_ed_rel, collect two lists of words that we would tag \n",
    "# as *Dish* based on the *Edit Distance* between our Dish Gazetteer and test_sentences.     \n",
    "gz_dish_25_words = ner_ed_rel(test_sentences, dish_hyponyms, threshold_percent=0.25)\n",
    "# 50% threshold\n",
    "gz_dish_50_words = ner_ed_rel(test_sentences, dish_hyponyms, threshold_percent=0.5)\n",
    "# Print the returned lists.\n",
    "print(\"Distance 25%\")\n",
    "print(gz_dish_25_words)\n",
    "print(\"Disance 50%\")\n",
    "print(gz_dish_50_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5 - TO DO (5)    \n",
    "Does relative thresholding better capture variations than not using a relative thresholding for the tag *Dish*? Give an example to justify your answer.     \n",
    "Yes. Chowder/chow_mein, french/french_toast, etc. It's still string matching at the end of the day so the semantics captured is a different question but in terms of capturing variations, relative thresholding does better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is the last notebook, hope you enjoyed them and learned some new things :) Best of luck with your projects!**\n",
    ":) Thank you <3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Signature\n",
    "\n",
    "I, -------Rupsi Kaushik--------------, declare that the answers provided in this notebook are my own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
